from langchain_google_genai import GoogleGenerativeAI
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# Import functions for building standard and experimental RAG models
from utils.build_rag_from_books import build_standard_rag
from utils.build_rag_from_books_and_volatility3_data import build_experimental_forensic_rag

# Function to initialize a retrieval-augmented generation (RAG) chat session
def chat_rag(retriever, llm_option):
    """
    Creates a retrieval-augmented chat system using a specified retriever and Gemini LLM.

    :param llm_option: Differ between Google Gemini and OpenAI to select the correct LangChain Chat Model.
    :param retriever: A retriever object to fetch relevant documents.
    :return: A retrieval-based chat chain for answering queries.
    """
    # Initialize the LLM model
    if llm_option.startswith("gemini"):
        llm = GoogleGenerativeAI(model=llm_option)
    elif llm_option == "chatgpt-4o-latest" or llm_option == "gpt-3.5-turbo" or llm_option == "o1-preview":
        llm = ChatOpenAI(model=llm_option)

    # Define the system prompt for forensic memory analysis
    system_prompt = (
        "You are a forensic RAM Analyst Assistant. Traverse the JSON Tree and help to find an intruder."
        "Context: {context}"
    )

    # Create the structured prompt template
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{input}"),
        ]
    )

    # Create the document processing and retrieval chain
    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    chain = create_retrieval_chain(retriever, question_answer_chain)

    return chain

# Function to process user queries and retrieve responses
def answer_query(api_key, llm_option, embedding_option, standard_or_experimental, query):
    """
    Answers a user query by selecting either a standard or experimental RAG model.

    :param api_key: API key for LLM.
    :param llm_option: Selection if Google GenAI or OpenAI Model.
    :param embedding_option: Selection for Embedding.
    :param standard_or_experimental: Specifies whether to use 'standard' or 'experimental' RAG.
    :param query: The user query to process.
    :return: The response generated by the RAG model.
    """
    if standard_or_experimental == "standard":
        retriever = build_standard_rag(api_key, llm_option, embedding_option)
    elif standard_or_experimental == "experimental":
        retriever = build_experimental_forensic_rag(api_key, llm_option, embedding_option)
    chain = chat_rag(retriever, llm_option) # Initialize the RAG chat system
    result = chain.invoke({"input": query}) # Process the query
    return result
